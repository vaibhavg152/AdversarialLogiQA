# -*- coding: utf-8 -*-
"""Adding_Irrelevant_Info.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17uu6KMnvGxdWhkLm_bcTt8s6pD2fojcZ
"""

import pandas as pd
import datasets
from datasets import load_dataset
import nltk
from nltk import sent_tokenize, word_tokenize
from nltk.corpus import brown
from mosestokenizer import MosesDetokenizer
import random
from enum import Enum
import os
import sys
from collections import defaultdict

# nltk.download('perluniprops')

# Download and preprocess Shakespearean text
dataset = load_dataset('tiny_shakespeare')['train']
# nltk.download('punkt')
shakespearean_text = [text.replace('\n', ' ') for text in sent_tokenize(dataset['text'][0])]

# Download and preprocess Brown corpus
# nltk.download('brown')
# nltk.download('perluniprops')
mdetok = MosesDetokenizer('en')
brown_natural = [mdetok(' '.join(sent).replace('``', '').replace("''", '').replace('`', "").split())  for sent in brown.sents()]

# Ensure that Train, Eval and Test files are in the same directory if not downloading
# !wget https://raw.githubusercontent.com/lgw863/LogiQA-dataset/master/Train.txt -O Train.txt
# !wget https://raw.githubusercontent.com/lgw863/LogiQA-dataset/master/Eval.txt -O Eval.txt
# !wget https://raw.githubusercontent.com/lgw863/LogiQA-dataset/master/Test.txt -O Test.txt

class Position(Enum):
  PREPEND = 1
  IN_BETWEEN = 2
  APPEND = 3

def group_sent_by_length(sentences):
  sentence_groups = defaultdict(list)
  for text in sentences:
    sentence_groups[len(word_tokenize(text))].append(text)
  keys = sorted(sentence_groups.keys())
  new_dict = {x:sentence_groups[x] for x in keys}
  return new_dict

def prepare_irrelavent_data(filename, pos: Position, irrelevant_data_source, 
                            out_file_name, irrelevant_data_length = None):
  
  with open(filename, 'r') as f:
    lines = f.readlines()
  
  assert len(lines)%8==0
  n_examples = len(lines) // 8

  out_file_name = '{}_logiqa/{}'.format(out_file_name, filename.split('/')[-1])
  # print(out_file_name)
  if os.path.exists(out_file_name):
    os.remove(out_file_name)

  for i in range(n_examples):
    label_str = lines[i*8+1]
    context = lines[i*8+2].strip()
    question = lines[i*8+3]
    answers = lines[i*8+4 : i*8+8]

    if irrelevant_data_length is None:
      random_length = random.choice(irrelevant_data_source.keys())
      irrelevant_data = random.choice(irrelevant_data_source[random_length])
    else:
      # print([(x, len(irrelevant_data_source[x])) for x in irrelevant_data_source])
      irrelevant_data = random.choice(irrelevant_data_source[irrelevant_data_length])
    
    if pos == Position.PREPEND:
      context = irrelevant_data + ' ' + context + '\n'
    elif pos == Position.IN_BETWEEN:
      sentences = sent_tokenize(context)
      sentences.insert(random.randint(0, len(sentences)), irrelevant_data)
      context = ' '.join(sentences) + '\n'
    else:
      context += ' ' + irrelevant_data + '\n'

    with open(out_file_name, 'a') as of:
      of.write('\n')
      of.write(label_str)
      of.write(context)
      of.write(question)
      for i in range(4):
        of.write(answers[i])
    # print(out_file_name)

shakespearean_text = group_sent_by_length(shakespearean_text)
brown_natural = group_sent_by_length(brown_natural)
data_sources = {'Brown': brown_natural, 'Shakespeare': shakespearean_text}
logiQA_files = ['logiqa_data/Eval.txt', 'logiqa_data/Test.txt']
position = Position.IN_BETWEEN

irrelevant_data_word_length = int(sys.argv[1])

print("length =", irrelevant_data_word_length)
for file in logiQA_files:
  for data_source_name, source in data_sources.items():
    prepare_irrelavent_data(file, position, source,
                            data_source_name, irrelevant_data_word_length)
