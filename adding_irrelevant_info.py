# -*- coding: utf-8 -*-
"""Adding_Irrelevant_Info.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17uu6KMnvGxdWhkLm_bcTt8s6pD2fojcZ
"""

import pandas as pd
import datasets
from datasets import load_dataset
import nltk
from nltk import sent_tokenize, word_tokenize
from nltk.corpus import brown
from mosestokenizer import MosesDetokenizer
import random
from enum import Enum
import os
import sys
from collections import defaultdict

nltk.download('perluniprops')
from nltk.tokenize.moses import MosesDetokenizer

# Download and preprocess Shakespearean text
dataset = load_dataset('tiny_shakespeare')['train']
nltk.download('punkt')
shakespearean_text = [text.replace('\n', ' ') for text in sent_tokenize(dataset['text'][0])]

# Download and preprocess Brown corpus
nltk.download('brown')
nltk.download('perluniprops')
mdetok = MosesDetokenizer('en')
brown_natural = [mdetok(' '.join(sent).replace('``', '').replace("''", '').replace('`', "").split())  for sent in brown.sents()]

# Ensure that Train, Eval and Test files are in the same directory if not downloading
# !wget https://raw.githubusercontent.com/lgw863/LogiQA-dataset/master/Train.txt -O Train.txt
# !wget https://raw.githubusercontent.com/lgw863/LogiQA-dataset/master/Eval.txt -O Eval.txt
# !wget https://raw.githubusercontent.com/lgw863/LogiQA-dataset/master/Test.txt -O Test.txt

class Position(Enum):
  PREPEND = 1
  IN_BETWEEN = 2
  APPEND = 3

def group_sent_by_length(sentences):
  sentence_groups = defaultdict(list)
  for text in sentences:
    sentence_groups[len(word_tokenize(text))].append(text)

  return sentence_groups

def prepare_irrelavent_data(filename, pos: Position, irrelevant_data_source, 
                            out_file_name, irrelevant_data_length = None):
  
  with open(filename, 'r') as f:
    lines = f.readlines()
  
  assert len(lines)%8==0
  n_examples = len(lines) // 8

  out_file_name = filename[:filename.index('.')] + '_' + out_file_name + '.txt'

  if os.path.exists(out_file_name):
    os.remove(out_file_name)

  for i in range(n_examples):
    label_str = lines[i*8+1]
    context = lines[i*8+2].strip()
    question = lines[i*8+3]
    answers = lines[i*8+4 : i*8+8]

    if irrelevant_data_length is None:
      random_length = random.choice(irrelevant_data_source.keys())
      irrelevant_data = random.choice(irrelevant_data_source[random_length])
    else:
      irrelevant_data = random.choice(irrelevant_data_source[irrelevant_data_length])
    
    if pos == Position.PREPEND:
      context = irrelevant_data + ' ' + context + '\n'
    elif pos == Position.IN_BETWEEN:
      sentences = sent_tokenize(context)
      sentences.insert(random.randint(0, len(sentences)), irrelevant_data)
      context = ' '.join(sentences) + '\n'
    else:
      context += ' ' + irrelevant_data + '\n'

    with open(out_file_name, 'a') as of:
      of.write('\n')
      of.write(label_str)
      of.write(context)
      of.write(question)
      for i in range(4):
        of.write(answers[i])

shakespearean_text = group_sent_by_length(shakespearean_text)
brown_natural = group_sent_by_length(brown_natural)
data_sources = {'shakespeare': shakespearean_text, 'brown': brown_natural}
logiQA_files = ['Eval.txt', 'Test.txt']
position = Position.IN_BETWEEN

irrelevant_data_word_length = sys.args[1]

for file in logiQA_files:
  for data_source_name in data_sources:
    prepare_irrelavent_data('logiqa_data/{}_'.format(irrelevant_data_word_length)+file, position, data_sources[data_source_name], 
                            data_source_name, irrelevant_data_word_length)
